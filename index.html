<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Cross-Modal Retrieval: A Systematic Review of Methods and Future Directions</title>

    <script>
        var task_map = {
            "simple-object-manipulation": "simple_object_manipulation",
            "visual-goal-reaching": "visual_goal_reaching",
            "novel-concept-grounding": "novel_concept_grounding",
            "one-shot-video-imitation": "one_shot_video_imitation",
            "visual-constraint-satisfaction": "visual_constraint_satisfaction",
            "visual-reasoning": "visual_reasoning"
        };

        function updateDemoVideo(category) {
            // var demo = document.getElementById("single-menu-demos").value;
            var task = document.getElementById(category + "-menu-tasks").value;
            var inst = document.getElementById(category + "-menu-instances").value;

            console.log(task_map[category], task, inst)

            var video = document.getElementById(category + "-single-task-video");
            video.src = "assets/videos/demos/" +
                task_map[category] +
                "/" +
                task +
                "/" +
                inst +
                ".mp4";
            video.playbackRate = 2.0;
            video.play();
        }
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">Cross-Modal Retrieval: A Systematic Review of Methods and Future Directions</h1>
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a target="_blank" href="">Lei&#160;Zhu</a><sup>1</sup>,
                <a target="_blank" href="">Tianshi&#160;Wang</a><sup>1</sup>,
                <a target="_blank" href="">Fengling&#160;Li</a><sup>2</sup>,
                <a target="_blank" href="">Jingjing&#160;Li</a><sup>3</sup>,
                <br>
                <a target="_blank" href="">Zheng&#160;Zhang</a><sup>4</sup>,
                <a target="_blank" href="">Heng&#160;Tao&#160;Shen</a><sup>3</sup>
            </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>Shandong Normal University, </span>
                        <span class="author-block"><sup>2</sup>University of Technology Sydney, </span>
                        <span class="author-block"><sup>3</sup>University of Electronic Science and Technology of China, </span>
                        <span class="author-block"><sup>4</sup>Harbin Institute of Technology, Shenzhen </span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- TODO PDF Link. -->
                            <span class="link-block">
                <a target="_blank" href="http://arxiv.org/abs/2308.14263v2"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                            <!-- Code Link. -->
                            <span class="link-block">
                <a target="_blank" href="https://github.com/BMC-SDNU/Cross-Modal-Retrieval"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
                <a target="_blank" href="https://github.com/BMC-SDNU/Cross-Modal-Retrieval"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-robot"></i>
                  </span>
                  <span>Model</span>
                </a>
                <a target="_blank" href="https://github.com/BMC-SDNU/Cross-Modal-Retrieval"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-network-wired"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p style="font-size: 130%">
                        With the exponential surge in diverse multi-modal data, traditional uni-modal retrieval methods struggle to meet the needs of users seeking access to data across various modalities. 
                        To address this, cross-modal retrieval has emerged, enabling interaction across modalities, facilitating semantic matching, and leveraging complementarity and consistency between different modal data. 
                        Although prior literature has reviewed the field of cross-modal retrieval, it suffers from numerous deficiencies in terms of timeliness, taxonomy, and comprehensiveness. 
                        This paper conducts a comprehensive review of cross-modal retrieval's evolution, spanning from shallow statistical analysis techniques to vision-language pre-training models. 
                        Commencing with a comprehensive taxonomy grounded in machine learning paradigms, mechanisms, and models, the paper delves deeply into the principles and architectures underpinning existing cross-modal retrieval methods. 
                        Furthermore, it offers an overview of widely-used benchmarks, metrics, and performances. 
                        Lastly, the paper probes the prospects and challenges that confront contemporary cross-modal retrieval, while engaging in a discourse on potential directions for further progress in the field.
                    </p>
                    <img src="assets/images/intro.jpg" class="interpolation-image"
                    alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>
                    <br>
                    <p align="center" style="font-weight: bold; font-size: 130%"> Illustration of cross-modal retrieval.</p>
                    <br>
                </div>
            </div>
        </div>
    </div>
</section>

<!--Model-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">The heterogeneous gap of multi-modal data</span></h2>
                    <p style="font-size: 130%">
                        Cross-modal retrieval stands as a pivotal domain within multi-media retrieval, poised with immense potential in the realm of artificial intelligence. 
                        Its purpose is to glean semantically pertinent information from disparate modalities, leveraging given modal cues like text, image, or video.
                        Nonetheless, the landscape of cross-modal retrieval is rugged, the paramount among which is gauging content affinity amidst heterogeneous modal dataâ€”a conundrum often dubbed the heterogeneous modality gap.
                    </p>
                    <br>
                    <img src="assets/images/modality_gap.jpg" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 60%;"/>
                    <br>
                <p align="center" style="font-weight: bold; font-size: 130%">Diagram of heterogeneous modality gap. </p>
                </div>
            </div>
        </div>
    </div>
</section>

<!--Model-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">The distinct challenges across heterogeneous modalities</span></h2>
                        <p style="font-size: 130%">
                            As cross-modal retrieval has advanced, it now extends beyond traditional text-image retrieval to encompass a broader array of data modalities and retrieval tasks. 
                            This section offers a comprehensive review of cross-modal retrieval methods that involve modalities beyond text-image, including text-video, text-audio, image-audio, image3D, and more. 
                            We explore the unique challenges associated with these extended modalities, offering insights into the specific techniques and architectures designed to handle the complexities of each combination.
                        </p>
                    <br>
                    <img src="assets/images/modality_challenge.jpg" class="interpolation-image"
                    alt="" style="display: block; margin-left: auto; margin-right: auto; width: 50%;"/>
                    <br>
                <p align="center" style="font-weight: bold; font-size: 130%">Diagram of distinct challenges across heterogeneous modalities. </p>
                </div>
            </div>
        </div>
    </div>
</section>


<!--Experiments-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvima">Text-image cross-modal retrieval</span></h2>

                    <p style="font-size: 125%">
                        We investigate three main questions during experiments:
                        <ul style="font-size: 125%; padding-left: 5%">
                        <li><span style="font-weight: bold;">(1) Effectiveness.</span> We wonder the imitation learning performance of cross-modal-retrieval by training it on the given demonstration data. </li>
                        <li><span style="font-weight: bold;">(2) Zero-shot Generalization.</span> We focus on generalization on unseen tasks. In other words, we wonder how the model will behave given unseen vision contexts like different objects, even with unseen instructions.</li> 
                        <li><span style="font-weight: bold;">(3) Ablation Studies.</span> We further explore the essential factors that matter in adapting VLMs to robot control policy in the framework of cross-modal-retrieval.</li>
                    </ul>
                    </p>
                    <br>
                    <br>

                    <h3 class="title is-4"><span
                            class="dvima">(1) Imitation Performance</span></h3>

                    <img src="assets/images/evaluation.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto; width: 80%;"/>
                    <br>
                    <span style="font-size: 125%">
                        <span style="font-weight: bold">Table1: The imitation performance on various settings, all results are reported using the best-behaved model checkpoints.</span> <i>(Full)</i> and <i>(Lang)</i> denote if the model is trained using unpaired vision data (i.e., vision data without language pairs); <i>Freeze-emb</i> refers to freezing the embedding layer of the fusion decoder; <i>Enriched</i> denote using GPT-4 enriched instructions. The <span style="color: #A0A0A0">gray</span> rows denote numerical results evaluated by our re-trained model. We re-implement RT-1 and take the original code of HULC provided by <a href="https://github.com/lukashermann/hulc">Mees et al</a>. All other results are reported by <a href="https://github.com/lukashermann/hulc">Mees et al</a>.
                    </span>
                    <br>
                    <br>
                    <br>

                    <h3 class="title is-4"><span
                        class="dvima">(2) Zero-shot Generalization</span></h3>
                    <p style="font-size: 125%">
                        we evaluate two aspects of generalization for our cross-modal-retrieval: vision and language. 

                        For vision generalization, we train models on splits A, B, and C and test on split D, which presents a different vision context. Our method significantly outperforms baselines in this vision generalization scenario (<i>ABC &#8594; D</i>), as shown in the above Table 1. Regarding language generalization, we enrich the language setting by generating 50 synonymous instructions for each task using GPT-4. We then randomly sample instructions during evaluation. Our method exhibits superior performance compared to all baselines in this language generalization setting.

                        It's important to note that the success rate of our method on subsequent tasks shows a more noticeable drop compared to HULC. This may be due to our approach directly using word tokens as input during training, which can result in larger variations for synonymous sentences compared to HULC using a frozen sentence model for embedding instructions. To address this, we freeze the embedding layer of the feature fusion decoder in our method, leading to improved generalization and reduced performance drop.
                    </p>
                    <br>
                    <br>
                    <h3 class="title is-4"><span
                            class="dvima">(3) Ablation Studies</span></h3>
                    <p style="font-size: 125%">
                        We conduct ablation studies for cross-modal-retrieval to answer the following questions:
                        <ul style="font-size: 125%; padding-left: 5%">
                            <li>
                                1. How does cross-modal-retrieval perform with different heads? 
                            </li>
                            <li>
                                2. Does vision-language pre-training improve downstream robotic tasks? 
                            </li>
                            <li>
                                3. How do critical factors in vision-language pre-training affect robotic tasks?
                            </li>
                        </ul>
                    </p>
                    <br>
                    <img src="assets/images/vlms.png" class="interpolation-image"
                    alt="" style="display: block; margin-left: auto; margin-right: auto; width: 80%;"/>
                    <br>
                    <span style="font-size: 125%">
                        <span style="font-weight: bold">Table2: Variants of VLMs tested.</span> <i>Pre-train</i> denotes the original performance of VLM on the pre-training VL dataset, <i>Best Avg. Len.</i> denotes the best performance of the average success length of VLMs within 5 epochs, and <i>Mean Avg. Len.</i> denotes the mean performance of the average success length of VLMs of last 3 epochs on CALVIN.</a>.
                    <br>
                    <br>
                    <div class="columns">
                        <div class="column has-text-left">
                            <h3 class="title is-5">(a) Various policy head</h3>
                            <img src="assets/images/policy_head.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                        </div>

                        <div class="column has-text-left">
                            <h3 class="title is-5">(b) Different training paradigms</h3>
                            <img src="assets/images/train_abl.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                        </div>

                        <div class="column has-text-left">
                            <h3 class="title is-5">(c) Open loop control</h3>
                            <img src="assets/images/open_loop.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                        </div>
                    </div>
                    <br>
                    <span style="font-size: 100%">
                        We observe also that 1) policy heads with history encoding performs the best and <i>GPT</i> and <i>LSTM</i> behaves similar under the framework of our cross-modal-retrieval; 2) tuning on the VL model itself on robotic tasks is indispensable due to limited capacity of the policy head and vision-langauge pre-training crucially improves the downstream robotic manipulation by a large margin; and 3) a larger model which usually results in better VL performance, achieves much higher performance, indicating that a larger VLM can be more data-efficient. 
                    </span>

                    <br>
                    <br>
                </div>
            </div>

        </div>
    </div>
</section>

<!--Model-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">Text-image cross-modal retrieval</span></h2>
                    <br>
                    <img src="assets/images/within_text_image.png" class="interpolation-image"
                    alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>
                    <br>
                    <span style="font-size: 130%">
                <span style="font-weight: bold">A compilation of representative text-image cross-modal retrieval methods. </span>Below, CCA stands for canonical correlation analysis, CNN-RNN stands for convolutional neural network and recurrent neural network, GAN stands for generative adversarial network, GNN stands for graph neural network, and VLP model stands for vision-language pre-training model.</span>
                </div>
            </div>
        </div>
    </div>
</section>

<!--Model-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">Cross-modal retrieval beyond text-image</span></h2>
                    <br>
                    <img src="assets/images/beyond_text_image.png" class="interpolation-image"
                    alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>
                    <br>
                    <span style="font-size: 130%">
                <span style="font-weight: bold">A compilation of representative cross-modal retrieval methods beyond text-image retrieval. </span>The abbreviations used remain consistent with the previous definitions.</span>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-widescreen">

        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvima">Conclusion and Future Work</span></h2>
                    <div style="font-size: 125%;">
                        <span style="font-size: 100%;">
                            This paper explores the potential of pre-trained vision-language models in advancing language-conditioned
                            robotic manipulation. Our proposed cross-modal-retrieval, based on the pre-trained OpenFlamingo model, showcases state-of-the-art
                            performance on a benchmark dataset. Moreover, our experimental findings highlight the benefits of pre-trained models in terms of data efficiency and zero-shot generalization ability.
                            This research contributes to the ongoing efforts to develop intelligent robotic systems that can seamlessly understand and respond to human language instructions, paving the way for more intuitive and efficient human-robot collaboration. Due to the lack of real-robot data, this paper does not deploy on real-world robotics. 
                            To our delight, recent progress on large-scale real robotics data (<a href="https://robotics-transformer-x.github.io/">Open X-Embodiment</a>) has shown the potential of fine-tuning large VLMs for real robots, and the most interesting future work is to see how cross-modal-retrieval will behave in real-world tasks combined with such amount of data.
                        </span>
                    </div>
                </div>
            </div>

        </div>
    </div>
</section>

<section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@article{li2023cross,
  title={Cross-modal retrieval: a systematic review of methods and future directions},
  author={Li, Fengling and Zhu, Lei and Wang, Tianshi and Li, Jingjing and Zhang, Zheng and Shen, Heng Tao},
  journal={arXiv preprint arXiv:2308.14263},
  year={2023}
}</code></pre>
    </div>
</section>

<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column">
                <div class="content has-text-centered">
                    <p>
                        Website template borrowed from <a
                            href="https://vimalabs.github.io/">VIMA</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
