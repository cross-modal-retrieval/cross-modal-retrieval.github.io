<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Cross-Modal Retrieval: A Systematic Review of Methods and Future Directions</title>

    <script>
        var task_map = {
            "simple-object-manipulation": "simple_object_manipulation",
            "visual-goal-reaching": "visual_goal_reaching",
            "novel-concept-grounding": "novel_concept_grounding",
            "one-shot-video-imitation": "one_shot_video_imitation",
            "visual-constraint-satisfaction": "visual_constraint_satisfaction",
            "visual-reasoning": "visual_reasoning"
        };

        function updateDemoVideo(category) {
            // var demo = document.getElementById("single-menu-demos").value;
            var task = document.getElementById(category + "-menu-tasks").value;
            var inst = document.getElementById(category + "-menu-instances").value;

            console.log(task_map[category], task, inst)

            var video = document.getElementById(category + "-single-task-video");
            video.src = "assets/videos/demos/" +
                task_map[category] +
                "/" +
                task +
                "/" +
                inst +
                ".mp4";
            video.playbackRate = 2.0;
            video.play();
        }
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">Cross-Modal Retrieval: A Systematic Review of Methods and Future Directions</h1>
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a target="_blank" href="">Lei&#160;Zhu</a><sup>1</sup>,
                <a target="_blank" href="">Tianshi&#160;Wang</a><sup>1</sup>,
                <a target="_blank" href="">Fengling&#160;Li</a><sup>2</sup>,
                <a target="_blank" href="">Jingjing&#160;Li</a><sup>3</sup>,
                <br>
                <a target="_blank" href="">Zheng&#160;Zhang</a><sup>4</sup>,
                <a target="_blank" href="">Heng&#160;Tao&#160;Shen</a><sup>3</sup>
            </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>Shandong Normal University, </span>
                        <span class="author-block"><sup>2</sup>University of Technology Sydney, </span>
                        <span class="author-block"><sup>3</sup>University of Electronic Science and Technology of China, </span>
                        <span class="author-block"><sup>4</sup>Harbin Institute of Technology, Shenzhen </span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- TODO PDF Link. -->
                            <span class="link-block">
                <a target="_blank" href="http://arxiv.org/abs/2308.14263v2"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                            <!-- Code Link. -->
                            <span class="link-block">
                <a target="_blank" href="https://github.com/BMC-SDNU/Cross-Modal-Retrieval"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
                <a target="_blank" href="https://github.com/BMC-SDNU/Cross-Modal-Retrieval"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-robot"></i>
                  </span>
                  <span>Model</span>
                </a>
                <a target="_blank" href="https://github.com/BMC-SDNU/Cross-Modal-Retrieval"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-network-wired"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p style="font-size: 130%">
                        With the exponential surge in diverse multi-modal data, traditional uni-modal retrieval methods struggle to meet the needs of users seeking access to data across various modalities. 
                        To address this, cross-modal retrieval has emerged, enabling interaction across modalities, facilitating semantic matching, and leveraging complementarity and consistency between different modal data. 
                        Although prior literature has reviewed the field of cross-modal retrieval, it suffers from numerous deficiencies in terms of timeliness, taxonomy, and comprehensiveness. 
                        This paper conducts a comprehensive review of cross-modal retrieval's evolution, spanning from shallow statistical analysis techniques to vision-language pre-training models. 
                        Commencing with a comprehensive taxonomy grounded in machine learning paradigms, mechanisms, and models, the paper delves deeply into the principles and architectures underpinning existing cross-modal retrieval methods. 
                        Furthermore, it offers an overview of widely-used benchmarks, metrics, and performances. 
                        Lastly, the paper probes the prospects and challenges that confront contemporary cross-modal retrieval, while engaging in a discourse on potential directions for further progress in the field.
                    </p>
                    <img src="assets/images/intro.jpg" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>
                    <p align="center" style="font-weight: bold; font-size: 130%; margin-top: 10px"> Illustration of cross-modal retrieval.</p>
                </div>
            </div>
        </div>
    </div>
</section>

<!--Model-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3">1  Multi-modal data</h2>
                    <h3 class="title is-4"><span class="dvima">1.1  The heterogeneous gap of multi-modal data</span></h3>
                    <p style="font-size: 130%; margin-bottom: 20px; text-align: justify">
                        Cross-modal retrieval stands as a pivotal domain within multi-media retrieval, poised with immense potential in the realm of artificial intelligence. 
                        Its purpose is to glean semantically pertinent information from disparate modalities, leveraging given modal cues like text, image, or video.
                        Nonetheless, the landscape of cross-modal retrieval is rugged, the paramount among which is gauging content affinity amidst heterogeneous modal dataâ€”a conundrum often dubbed the heterogeneous modality gap.
                    </p>
                    <img src="assets/images/modality_gap.jpg" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 60%;"/>
                    <p align="center" style="font-weight: bold; font-size: 130%; margin-top: 10px; margin-bottom: 50px;">Diagram of heterogeneous modality gap. </p>
                    <h3 class="title is-4"><span class="dvima">1.2  The distinct challenges across heterogeneous modalities</span></h3>
                        <p style="font-size: 130%; margin-bottom: 20px; text-align: justify">
                            As cross-modal retrieval has advanced, it now extends beyond traditional text-image retrieval to encompass a broader array of data modalities and retrieval tasks. 
                            This section offers a comprehensive review of cross-modal retrieval methods that involve modalities beyond text-image, including text-video, text-audio, image-audio, image3D, and more. 
                            We explore the unique challenges associated with these extended modalities, offering insights into the specific techniques and architectures designed to handle the complexities of each combination.
                        </p>
                    <img src="assets/images/modality_challenge.jpg" class="interpolation-image"
                    alt="" style="display: block; margin-left: auto; margin-right: auto; width: 50%;"/>
                <p align="center" style="font-weight: bold; font-size: 130%; margin-top: 10px;">Diagram of distinct challenges across heterogeneous modalities. </p>
                </div>
            </div>
        </div>
    </div>
</section>

<!--Experiments-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvima">2  Text-image cross-modal retrieval</span></h2>
                    <p style="font-size: 125%; margin-bottom: 20px; text-align: justify">
                        Current cross-modal retrieval methods are categorized into five overarching categories: unsupervised real-value retrieval, supervised real-value retrieval, unsupervised hashing retrieval, supervised hashing retrieval, and cross-modal retrieval under special scenarios. 
                        Each of these overarching categories is subdivided based on specific technical architectures or scenarios:
                    </p>
                    <img src="assets/images/within_text_image.png" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>
                    <p style="font-size: 125%; margin-top: 10px; margin-bottom: 50px; text-align: justify">
                    <span style="font-weight: bold">A compilation of representative text-image cross-modal retrieval methods. </span>Below, CCA stands for canonical correlation analysis, CNN-RNN stands for convolutional neural network and recurrent neural network, GAN stands for generative adversarial network, GNN stands for graph neural network, and VLP model stands for vision-language pre-training model.</p>
                    <br>
                    <br>
                    <h3 class="title is-4"><span
                            class="dvima">2.1  Unsupervised real-value retrieval</span></h3>

                    <img src="assets/images/UR.jpg" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>
                    <span style="font-size: 100%">
                        <p align="center" style="font-weight: bold; font-size: 130%; margin-top: 10px; margin-bottom: 50px;">The evolutionary tree of representative unsupervised real-value retrieval methods.</p>
                    </span>
                    <h3 class="title is-4"><span
                            class="dvima">2.2  Supervised real-value retrieval</span></h3>

                    <img src="assets/images/SR.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>
                    <span style="font-size: 100%">
                        <p align="center" style="font-weight: bold; font-size: 130%; margin-top: 10px; margin-bottom: 50px;">The evolutionary tree of representative supervised real-value retrieval methods.</p>
                    </span>
                                        <h3 class="title is-4"><span
                            class="dvima">2.3  Unsupervised hashing retrieval</span></h3>

                    <img src="assets/images/UH.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>
                    <span style="font-size: 100%">
                        <p align="center" style="font-weight: bold; font-size: 130%; margin-top: 10px; margin-bottom: 50px;">The evolutionary tree of representative unsupervised hashing retrieval methods.</p>
                    </span>
                                        <h3 class="title is-4"><span
                            class="dvima">2.4  Supervised hashing retrieval</span></h3>

                    <img src="assets/images/SH.jpg" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>
                    <span style="font-size: 100%">
                        <p align="center" style="font-weight: bold; font-size: 130%; margin-top: 10px; margin-bottom: 50px;">The evolutionary tree of representative supervised hashing retrieval methods.</p>
                    </span>
                                        <h3 class="title is-4"><span
                            class="dvima">2.5  Cross-modal retrieval under special scenarios</span></h3>
                    <img src="assets/images/SS.jpg" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto; width: 60%;"/>
                    <span style="font-size: 100%">
                        <p align="center" style="font-weight: bold; font-size: 130%; margin-top: 10px;">A compilation of representative methods for cross-modal retrieval under special scenarios.</p>
                    </span>
                </div>
            </div>
        </div>
    </div>
</section>

<!--Model-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">3  Cross-modal retrieval beyond text-image</span></h2>
                    <img src="assets/images/beyond_text_image.png" class="interpolation-image"
                    alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>
                    <span style="font-size: 100%">
                <p align="center" style="font-weight: bold; font-size: 130%; margin-top: 10px;">A compilation of representative cross-modal retrieval methods beyond text-image retrieval. </p></span>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvima">5  Conclusion</span></h2>
                    <div style="font-size: 125%;">
                        <p style="font-size: 100%; text-align: justify">
                            Cross-modal retrieval addresses the growing need for accessing and utilizing diverse multi-modal data. 
                            The evolution of research in this field has improved the accuracy, stability, and scalability of retrieval systems. 
                            The paper presents a comprehensive taxonomy, reviews numerous papers, and provides insights into cross-modal retrieval methods and architectures. 
                            It also offers guidance on dataset selection and performance evaluation metrics. 
                            The paper explores opportunities, challenges, and future research directions, contributing to the understanding and development of cross-modal retrieval. 
                            Further exploration and innovation in this field are encouraged.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@article{li2023cross,
  title={Cross-modal retrieval: a systematic review of methods and future directions},
  author={Li, Fengling and Zhu, Lei and Wang, Tianshi and Li, Jingjing and Zhang, Zheng and Shen, Heng Tao},
  journal={arXiv preprint arXiv:2308.14263},
  year={2023}
}</code></pre>
    </div>
</section>

</body>
</html>
