<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Cross-Modal Retrieval: A Systematic Review of Methods and Future Directions</title>

    <script>
        var task_map = {
            "simple-object-manipulation": "simple_object_manipulation",
            "visual-goal-reaching": "visual_goal_reaching",
            "novel-concept-grounding": "novel_concept_grounding",
            "one-shot-video-imitation": "one_shot_video_imitation",
            "visual-constraint-satisfaction": "visual_constraint_satisfaction",
            "visual-reasoning": "visual_reasoning"
        };

        function updateDemoVideo(category) {
            // var demo = document.getElementById("single-menu-demos").value;
            var task = document.getElementById(category + "-menu-tasks").value;
            var inst = document.getElementById(category + "-menu-instances").value;

            console.log(task_map[category], task, inst)

            var video = document.getElementById(category + "-single-task-video");
            video.src = "assets/videos/demos/" +
                task_map[category] +
                "/" +
                task +
                "/" +
                inst +
                ".mp4";
            video.playbackRate = 2.0;
            video.play();
        }
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">Cross-Modal Retrieval: A Systematic Review of Methods and Future Directions</h1>
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a target="_blank" href="">Lei&#160;Zhu</a><sup>1</sup>,
                <a target="_blank" href="">Tianshi&#160;Wang</a><sup>1</sup>,
                <a target="_blank" href="">Fengling&#160;Li</a><sup>2</sup>,
                <a target="_blank" href="">Jingjing&#160;Li</a><sup>3</sup>,
                <br>
                <a target="_blank" href="">Zheng&#160;Zhang</a><sup>4</sup>,
                <a target="_blank" href="">Heng&#160;Tao&#160;Shen</a><sup>3</sup>
            </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>Shandong Normal University, </span>
                        <span class="author-block"><sup>2</sup>University of Technology Sydney, </span>
                        <span class="author-block"><sup>3</sup>University of Electronic Science and Technology of China, </span>
                        <span class="author-block"><sup>4</sup>Harbin Institute of Technology, Shenzhen </span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- TODO PDF Link. -->
                            <span class="link-block">
                                <a target="_blank" href="http://arxiv.org/abs/2308.14263v2" class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                <i class="ai ai-arxiv"></i>
                                </span>
                                <span>arXiv</span>
                                </a>
                                <a target="_blank" href="https://github.com/BMC-SDNU/Cross-Modal-Retrieval" class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="fas fa-robot"></i>
                                    </span>
                                    <span>Toolbox</span>
                                </a>
                            </span>
                            <!-- Code Link. -->
                            <span class="link-block">
                                <a target="_blank" href="https://github.com/BMC-SDNU/Cross-Modal-Retrieval" class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                <i class="fab fa-github"></i>
                                </span>
                                <span>Code</span>
                                </a>
                                <a target="_blank" href="https://github.com/BMC-SDNU/Cross-Modal-Retrieval" class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="fa fa-network-wired"></i>
                                    </span>
                                    <span>Data</span>
                                </a>
                            </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p style="font-size: 130%">
                        With the exponential surge in diverse multi-modal data, traditional uni-modal retrieval methods struggle to meet the needs of users seeking access to data across various modalities. 
                        To address this, cross-modal retrieval has emerged, enabling interaction across modalities, facilitating semantic matching, and leveraging complementarity and consistency between different modal data. 
                        Although prior literature has reviewed the field of cross-modal retrieval, it suffers from numerous deficiencies in terms of timeliness, taxonomy, and comprehensiveness. 
                        This project conducts a comprehensive review of cross-modal retrieval's evolution, spanning from shallow statistical analysis techniques to vision-language pre-training models. 
                        Commencing with a comprehensive taxonomy grounded in machine learning paradigms, mechanisms, and models, the project delves deeply into the principles and architectures underpinning existing cross-modal retrieval methods. 
                        Furthermore, it offers an overview of widely-used benchmarks, metrics, and performances. 
                        Lastly, the project probes the prospects and challenges that confront contemporary cross-modal retrieval, while engaging in a discourse on potential directions for further progress in the field.
                    </p>
                    <img src="assets/images/intro.jpg" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 80%;"/>
                    <p align="center" style="font-weight: bold; font-size: 130%; margin-top: 10px"> Illustration of cross-modal retrieval.</p>
                </div>
            </div>
        </div>
    </div>
</section>

<!--Model-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3">1 &nbsp;&nbsp; Multi-Modal Data</h2>
                    <h3 class="title is-4"><span class="dvima">1.1 &nbsp;&nbsp; The heterogeneous gap of multi-modal data</span></h3>
                    <p style="font-size: 130%; margin-bottom: 20px; text-align: justify">
                        Cross-modal retrieval stands as a pivotal domain within multi-media retrieval, poised with immense potential in the realm of artificial intelligence. 
                        Its purpose is to glean semantically pertinent information from disparate modalities, leveraging given modal cues like text, image, or video.
                        Nonetheless, the landscape of cross-modal retrieval is rugged, the paramount among which is gauging content affinity amidst heterogeneous modal dataâ€”a conundrum often dubbed the heterogeneous modality gap.
                    </p>
                    <img src="assets/images/modality_gap.jpg" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 48%;"/>
                    <p align="center" style="font-weight: bold; font-size: 130%; margin-top: 10px; margin-bottom: 50px;">Diagram of heterogeneous modality gap. </p>
                    <h3 class="title is-4"><span class="dvima">1.2 &nbsp;&nbsp; The distinct challenges across heterogeneous modalities</span></h3>
                        <p style="font-size: 130%; margin-bottom: 20px; text-align: justify">
                            As cross-modal retrieval has advanced, it now extends beyond traditional text-image retrieval to encompass a broader array of data modalities and retrieval tasks. 
                            We explore the unique challenges associated with these modalities, offering insights into the specific techniques and architectures designed to handle the complexities of each combination.
                        </p>
                    <img src="assets/images/modality_challenge.jpg" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 35%;"/>
                <p align="center" style="font-weight: bold; font-size: 130%; margin-top: 10px;">Diagram of distinct challenges across heterogeneous modalities. </p>
                </div>
            </div>
        </div>
    </div>
</section>

<!--Experiments-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">2 &nbsp;&nbsp; Text-Image Cross-Modal Retrieval</span></h2>
                    <p style="font-size: 130%; margin-bottom: 20px; text-align: justify">
                        Current cross-modal retrieval methods are categorized into five overarching categories: unsupervised real-value retrieval, supervised real-value retrieval, unsupervised hashing retrieval, supervised hashing retrieval, and cross-modal retrieval under special scenarios. 
                        Each of these overarching categories is subdivided based on specific technical architectures or scenarios:
                    </p>
                    <img src="assets/images/within_text_image.png" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 90%;"/>
                    <p style="font-size: 130%; margin-top: 10px; margin-bottom: 50px; text-align: justify">
                    <span style="font-weight: bold">A compilation of representative text-image cross-modal retrieval methods. </span>Above, CCA stands for canonical correlation analysis, CNN-RNN stands for convolutional neural network and recurrent neural network, GAN stands for generative adversarial network, GNN stands for graph neural network, and VLP model stands for vision-language pre-training model.</p>
                    <h3 class="title is-4"><span class="dvima">2.1 &nbsp;&nbsp; Unsupervised real-value retrieval</span></h3>
                    <p style="font-size: 130%; margin-bottom: 20px; text-align: justify">
                        Unsupervised real-value retrieval aims to leverage the cooccurrence of multi-modal data, such as text and images appearing together, to capture their semantic correlation.  
                        Depending on the design principles and learning strategies, it can be categorized into two types: early unsupervised realvalue retrieval and object-oriented image-text matching.<br>
                        <span style="font-weight: bold;">1) Early unsupervised real-value retrieval</span> is subdivided into CCA methods, topic model methods, and auto-encoder methods;<br>
                        <span style="font-weight: bold;">2) Object-oriented image-text matching</span> is subdivided into CNN-RNN methods, GNN methods, Transformer methods, VLP model methods, and cross-modal generation methods.
                    </p>
                    <img src="assets/images/UR.png" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>
                    <span style="font-size: 100%">
                        <p align="center" style="font-weight: bold; font-size: 130%; margin-top: 10px; margin-bottom: 50px;">The evolutionary tree of representative unsupervised real-value retrieval methods.</p>
                    </span>
                    <h3 class="title is-4"><span class="dvima">2.2 &nbsp;&nbsp; Supervised real-value retrieval</span></h3>
                    <p style="font-size: 130%; margin-bottom: 20px; text-align: justify">
                        Supervised real-value retrieval, benefiting from manual annotation, explores semantic association and category discrimination in multi-modal data to achieve cross-modal retrieval. 
                        It involves two primary approaches: shallow and deep supervised real-value retrieval, based on different learning principles.<br>
                        <span style="font-weight: bold;">1) Shallow supervised real-value retrieval</span> is subdivided into CCA methods, dictionary learning methods, feature mapping methods, topic model methods, and metric learning methods;<br>
                        <span style="font-weight: bold;">2) Deep supervised real-value retrieval</span> is subdivided into CNN-RNN methods, GAN methods, GNN methods, and Transformer methods.
                    </p>
                    <img src="assets/images/SR.jpg" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>
                    <span style="font-size: 100%">
                        <p align="center" style="font-weight: bold; font-size: 130%; margin-top: 10px; margin-bottom: 50px;">The evolutionary tree of representative supervised real-value retrieval methods.</p>
                    </span>
                    <h3 class="title is-4"><span class="dvima">2.3 &nbsp;&nbsp; Unsupervised hashing retrieval</span></h3>
                    <p style="font-size: 130%; margin-bottom: 20px; text-align: justify">
                        Unsupervised hashing retrieval, akin to unsupervised real-value retrieval, leverages co-occurring multi-modal data (e.g., text-image pairs) to capture semantic correlations. 
                        It is divided into shallow and deep types based on their principles.<br>
                        <span style="font-weight: bold;">1) Shallow unsupervised hashing retrieval</span> is subdivided into matrix factorization methods, spectral graph methods, metric learning methods, and quantization methods;<br>
                        <span style="font-weight: bold;">2) Deep unsupervised hashing retrieval</span> is subdivided into CNN-RNN methods, GAN methods, GNN methods, Transformer methods, and knowledge distillation methods.
                    </p>
                    <img src="assets/images/UH.jpg" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>
                    <span style="font-size: 100%">
                        <p align="center" style="font-weight: bold; font-size: 130%; margin-top: 10px; margin-bottom: 50px;">The evolutionary tree of representative unsupervised hashing retrieval methods.</p>
                    </span>
                    <h3 class="title is-4"><span class="dvima">2.4 &nbsp;&nbsp; Supervised hashing retrieval</span></h3>
                    <p style="font-size: 130%; margin-bottom: 20px; text-align: justify">
                        Supervised hashing retrieval, benefiting from manual annotation, effectively maps multi-modal data into a low-dimensional Hamming space for efficient search by exploiting category discrimination and semantic associations. 
                        It can be divided into shallow and deep supervised hashing retrieval, based on different principles.<br>
                        <span style="font-weight: bold;">1) Shallow supervised hashing retrieval</span> is subdivided into matrix factorization methods and feature mapping methods;<br>
                        <span style="font-weight: bold;">2) Deep supervised hashing retrieval</span> is subdivided into CNN-RNN methods, GAN methods, GNN methods, Transformer methods, memory network methods, and quantization methods.
                    </p>
                    <img src="assets/images/SH.jpg" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>
                    <span style="font-size: 100%">
                        <p align="center" style="font-weight: bold; font-size: 130%; margin-top: 10px; margin-bottom: 50px;">The evolutionary tree of representative supervised hashing retrieval methods.</p>
                    </span>
                    <h3 class="title is-4"><span class="dvima">2.5 &nbsp;&nbsp; Cross-modal retrieval under special scenarios</span></h3>
                    <p style="font-size: 130%; margin-bottom: 20px; text-align: justify">
                        The cross-modal retrieval methods mentioned earlier are based on ideal assumptions and are applicable to general retrieval scenarios. 
                        However, practical constraints such as incomplete data collection, annotation noise, and specific retrieval needs have led to the development of various crossmodal retrieval methods tailored to address issues encountered in special scenarios.
                    </p>
                    <img src="assets/images/SS.jpg" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 65%;"/>
                    <span style="font-size: 100%">
                        <p align="center" style="font-weight: bold; font-size: 130%; margin-top: 10px;">A compilation of representative methods for cross-modal retrieval under special scenarios.</p>
                    </span>
                </div>
            </div>
        </div>
    </div>
</section>

<!--Model-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">3 &nbsp;&nbsp; Cross-Modal Retrieval beyond Text-Image</span></h2>
                    <p style="font-size: 130%; margin-bottom: 20px; text-align: justify">
                        As cross-modal retrieval has advanced, it now extends beyond traditional text-image retrieval to encompass a broader array of data modalities and retrieval tasks. 
                        This project also offers a comprehensive review of cross-modal retrieval methods that involve modalities beyond text-image, including text-video, text-audio, image-audio, image3D, and more.
                        We explore the unique challenges associated with these extended modalities, offering insights into the specific techniques and architectures designed to handle the complexities of each combination.
                    </p>
                    <img src="assets/images/beyond_text_image.png" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 90%;"/>
                    <span style="font-size: 100%">
                       <p align="center" style="font-weight: bold; font-size: 130%; margin-top: 10px;">A compilation of representative cross-modal retrieval methods beyond text-image retrieval. </p>
                    </span>
                </div>
            </div>
        </div>
    </div>
</section>

<!--Experiments-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">4 &nbsp;&nbsp; Discussion and Outlook</span></h2>
                    <p style="font-size: 130%; margin-bottom: 5px; text-align: justify">
                        As cross-modal retrieval continues to progress, it has the potential to revolutionize multimedia information retrieval and enable more efficient and comprehensive access to various data sources. 
                        Several promising research directions include:
                    </p>
                    <p style="font-size: 130%; margin-bottom: 0px; text-align: justify">
                        &nbsp;&nbsp;&nbsp;&nbsp; 1) Efficient cross-modal semantic modeling;
                    </p>
                    <p style="font-size: 130%; margin-bottom: 0px; text-align: justify">
                        &nbsp;&nbsp;&nbsp;&nbsp; 2) Multi-modal distribution adaptation;
                    </p>
                    <p style="font-size: 130%; margin-bottom: 0px; text-align: justify">
                        &nbsp;&nbsp;&nbsp;&nbsp; 3) Uncertain multi-modal data modeling;
                    </p>
                    <p style="font-size: 130%; margin-bottom: 0px; text-align: justify">
                        &nbsp;&nbsp;&nbsp;&nbsp; 4) Continuous retrieval model updating;
                    </p>
                    <p style="font-size: 130%; margin-bottom: 0px; text-align: justify">
                        &nbsp;&nbsp;&nbsp;&nbsp; 5) Interactive and user-centric retrieval;
                    </p>
                    <p style="font-size: 130%; margin-bottom: 0px; text-align: justify">
                        &nbsp;&nbsp;&nbsp;&nbsp; 6) Ethical challenges in cross-modal retrieval;
                    </p>
                    <p style="font-size: 130%; margin-bottom: 0px; text-align: justify">
                        &nbsp;&nbsp;&nbsp;&nbsp; 7) Distributed cross-modal retrieval.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">5 &nbsp;&nbsp; Conclusion</span></h2>
                    <div style="font-size: 130%;">
                        <p style="font-size: 100%; text-align: justify">
                            Cross-modal retrieval addresses the growing need for accessing and utilizing diverse multi-modal data. 
                            The evolution of research in this field has improved the accuracy, stability, and scalability of retrieval systems. 
                            The project presents a comprehensive taxonomy, reviews numerous papers, and provides insights into cross-modal retrieval methods and architectures. 
                            It also offers guidance on dataset selection and performance evaluation metrics. 
                            The project explores opportunities, challenges, and future research directions, contributing to the understanding and development of cross-modal retrieval. 
                            Further exploration and innovation in this field are encouraged.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
        <h2 class="title">BibTeX</h2>
        <pre><code>
@article{li2023cross,
      title={Cross-modal retrieval: a systematic review of methods and future directions},
      author={Li, Fengling and Zhu, Lei and Wang, Tianshi and Li, Jingjing and Zhang, Zheng and Shen, Heng Tao},
      journal={arXiv preprint arXiv:2308.14263},
      year={2023}
      }
        </code></pre>
    </div>
</section>

</body>
</html>
